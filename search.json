[
  {
    "objectID": "notebooks/aizcorbe_example_3.2.html",
    "href": "notebooks/aizcorbe_example_3.2.html",
    "title": "Aizcorbe (2014), ex 3.2. Hedonic dummy variable method",
    "section": "",
    "text": "Example of dummy variable hedonic method (from Aizcorbe 2014, example 3.2)\nIn Example 3.2, constructed a sythentic scanner dataset\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/aizcorbe_example_2.2.html",
    "href": "notebooks/aizcorbe_example_2.2.html",
    "title": "Aizcorbe (2014), ex 2.2. Bilateral indices",
    "section": "",
    "text": "Example of bilateral indices with product churn (from Aizcorbe 2014, example 2.2)\nIn Example 2.2, Aizcorbe uses the DRAM dataset to demonstrate how to construct bilateral price indices with product churn.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Overview",
    "section": "",
    "text": "This is a personal site that I used to render experiments and workflows that relate to the price statistics domain within official statistics. It is meant to log various experiments and notes to myself in a reproducible manner. Anyone else can use this if they choose but it is not explicitly meant for any formal capacity.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/index.html#purpose-of-site",
    "href": "docs/index.html#purpose-of-site",
    "title": "Overview",
    "section": "",
    "text": "This is a personal site that I used to render experiments and workflows that relate to the price statistics domain within official statistics. It is meant to log various experiments and notes to myself in a reproducible manner. Anyone else can use this if they choose but it is not explicitly meant for any formal capacity.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html",
    "href": "docs/notes_on_methods/bilateral_indices.html",
    "title": "Notes on bilateral price index methods",
    "section": "",
    "text": "This page provides an overview of Bilateral Price Index methods. Several sources are used (included in the links to the page) to go over key concepts. Emprical examples are linked from this page or can be found in the main guide\n\n\n\nA price index provides an aggregate measure of price change for a particular product segment, industry, or overall economy. … [They compare] the cost of purchasing a set of goods at different points in time. This “set of goods” is often referred to as the “market basket” or the “bundle” of goods. - Aizcorbe (2014)",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#the-concept-of-price-index-methods",
    "href": "docs/notes_on_methods/bilateral_indices.html#the-concept-of-price-index-methods",
    "title": "Notes on bilateral price index methods",
    "section": "",
    "text": "A price index provides an aggregate measure of price change for a particular product segment, industry, or overall economy. … [They compare] the cost of purchasing a set of goods at different points in time. This “set of goods” is often referred to as the “market basket” or the “bundle” of goods. - Aizcorbe (2014)",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#laspeyres-index",
    "href": "docs/notes_on_methods/bilateral_indices.html#laspeyres-index",
    "title": "Notes on bilateral price index methods",
    "section": "Laspeyres index",
    "text": "Laspeyres index\nThe simplest formula (and one that is commonly used) is the Laspeyres index:\n\\(I^{L}_{0,1} = \\frac{\\sum_{m=1}^{M}(P_{m,1}Q_{m,0})}{\\sum_{m=1}^{M}(P_{m,0}Q_{m,0})}\\)\nWhere P’s and Q’s denote prices and quantities, and 0 and 1 denote two points in time.\nIf M goods are sold in both periods (note that an overlap is needed), we can compare the the cost of purchasing the same goods we bought in period 1 with a certain period in the future.\nWe can also write the Laspeyres as the weighted arithmetic average of the price change of the individual products in the index\n\\(I^L_{0,1} = \\sum_{m=1}^{M} (w_{m,0}\\frac{P_{m,1}}{P_{m,0}})\\) such that \\(w_{m,0} = (P_{m,0}Q_{m,0})/\\sum_{m=1}^{M}(P_{m,0}Q_{m,0})\\)\ngive the ratio of good m’s expenditure to total expenditure, or could also be considered the relative importance or share of the product. There are some key nuances with this approach:\n\nProducts sold in both periods are included in both periods, thus new products are omitted.\nWe fix the relative importance of the goods for both periods based on period 1 weight, thus we do not reflect changes in composition over time (substitution). This can be convenient as we need weights only for the base period.\n\nMost price indices are variants of the Laspeyres, such as the Lowe (which compare the prices from the current month with the previous month, but use weights from a year before that)",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#paashe-index",
    "href": "docs/notes_on_methods/bilateral_indices.html#paashe-index",
    "title": "Notes on bilateral price index methods",
    "section": "Paashe Index",
    "text": "Paashe Index\nSimilar to the Lasperyes, however uses a different basket (the one from the pricing period:\n\\(I^P_{0,1} = \\sum_{m=1}^M (P_{m,1}Q_{m,1}) / \\sum_{m=1}^M\n(P_{m,0}Q_{m,1})\\) The Paasche may also be expressed as a function of the weighted average (i.e. shares):\n\\(I^P_{0,1} = 1/ \\sum_{m=1}^M (w_{m,1} \\frac{P_{m,0}}{P_{m,1}})\\) such that \\(w_{m,1} = (P_{m,1}Q_{m,1})/ \\sum_{m=1}^M (w_{m,1}P_{m,0}Q_{m,1})\\)\nThe Laspeyres and Paasche include prices and quantities in both periods, and both use the same relatives with different expenditure shares.",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#fisher-index",
    "href": "docs/notes_on_methods/bilateral_indices.html#fisher-index",
    "title": "Notes on bilateral price index methods",
    "section": "Fisher Index",
    "text": "Fisher Index\nThe Fisher does a geometric average of the Laspeyres and the Paasche:\n\\(I^F_{0,1} = (I^L_{0,1}I^P_{0,1})^\\frac{1}{2}\\)\nThe Fisher thus uses expenditure from both periods and thus provides relative importance that are more closely aligned with the goods actually sold. As the Fisher satisfies homogeneity, symmetry, and the time reversal test (the price change from the base to the current should be the inverse of the current to the base) - thus it doesn’t matter what period is chosen as the base.",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#törnqvist-index",
    "href": "docs/notes_on_methods/bilateral_indices.html#törnqvist-index",
    "title": "Notes on bilateral price index methods",
    "section": "Törnqvist Index",
    "text": "Törnqvist Index\nSimilar to the Fisher, however it takes the average of the weights instead of averaging the two indices\nIn logged form:\n\\(lnI^T_{0,1}=\\sum^M_{m=1}(w_{m,0}+w_{m,1})/2(ln\\frac{P_{m,1}}{P_{m,0}})\\)",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#jevons-index",
    "href": "docs/notes_on_methods/bilateral_indices.html#jevons-index",
    "title": "Notes on bilateral price index methods",
    "section": "Jevons Index",
    "text": "Jevons Index\nThe Jevons is unweighted geometric mean. Similar to the Törnqvist, in logged form:\n\\(lnI^J_{0,1} = \\frac{1}{M} \\sum^M_{m=1}ln(P_{m,1}/P_{m,0})\\)\nThe Jevons takes the unweighted average by replacing the \\((w_{m,0}+w_{m,1})/2\\) with \\(1/M\\), thus giving each model equal weight",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/empirical_experiments/experiments_catalogue.html",
    "href": "docs/empirical_experiments/experiments_catalogue.html",
    "title": "Experiments catalogue",
    "section": "",
    "text": "This pages lists the experiments done by topic on specific topics related in some way to price statistics",
    "crumbs": [
      "Experiments catalogue"
    ]
  },
  {
    "objectID": "docs/empirical_experiments/experiments_catalogue.html#list-of-empirical-price-statistics-experiments",
    "href": "docs/empirical_experiments/experiments_catalogue.html#list-of-empirical-price-statistics-experiments",
    "title": "Experiments catalogue",
    "section": "List of empirical price statistics experiments",
    "text": "List of empirical price statistics experiments\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nAizcorbe (2014), ex 2.1. Bilateral fixed-base and chained indices\n\n\nbilateral index methods, fixed-base indices, chained indices\n\n\n\n\nAizcorbe (2014), ex 2.2. Bilateral indices\n\n\nbilateral index methods, product churn\n\n\n\n\nAizcorbe (2014), ex 3.2. Hedonic dummy variable method\n\n\nhedonic methods\n\n\n\n\nBenchmark test in file formats\n\n\ndata engineering, file format benchmarks\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Experiments catalogue"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html",
    "href": "docs/notes_on_methods/sampling_with_scanner.html",
    "title": "Sampling methods for scanner data",
    "section": "",
    "text": "Even when scanner data is available, not all data necessarily needs to be used to calculate elementary price indices. There are two sampling approaches that are typically used as they are quite effective ways to use a subset of the data but - the static and dynamic sample methods. A requirement for sampling is that each unique product is already classified to categories within which the elementary price indices are calculated.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#the-concept",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#the-concept",
    "title": "Sampling methods for scanner data",
    "section": "The concept",
    "text": "The concept\n\nThe fixed sample method simulates a traditional approach, similar to field collection. The sample is refreshed every year to stay representative and replacement is used to maintain the sample following a traditional methodology (except that as scanner data is available to evaluation, i.e. seeing all products and all transactions provides a lot of additional detail of what product to replace with).\nThe method is simple (in the sense that it follows a traditional methodology and can be combined with other data in a traditional way), however it is resource intensive as replacement must be dealt with manually (for example the Netherlands found it too costly to extend to 6 scanner retailers and shifted to the dynamic sample method). It is also typically used with only a small (although representative) sample of data. It is also similar to traditional methods so its easy to implement when starting out with scanner data.\nThe fixed sample method favors comparability over representativity as the same items are compared over time, however the representativity of the sample deteriorates over time:\n\nA consequence of the method is thus that only similar products are selected for most disappearing items (to avoid quality adjustment), which means the method is most applicable to situations where quality change is not a major factor in the category.\nThe method is also only applicable to categories with little churn as representativity of the products in sample will decline quickly if this is not the case. Both of these consequences are only partially mitigated by the annual product resampling. In this",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#step-by-step-process",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#step-by-step-process",
    "title": "Sampling methods for scanner data",
    "section": "Step-by-step process",
    "text": "Step-by-step process\nThe summary of the overall process is best summarized by the Eurostat (2017) guide, chapter 6:\n\nIn the static approach, a sample is drawn from year t and used for 12 months following December of year t. The sample is kept and replacements are made as needed.\n\nIn a more detailed sense, the following steps are used to select and maintain the sample:\n\nStep 1: For each category (typically COICOP 6) for which an elementary price index will be calculated, select a number of items (representative products) at the beginning of the year which were representative the previous year. For instance you can look at products that constituted the top 50% of annual turnover.\n\nSelect unique products (such as by European Article Number (EAN)) per supermarket chain and stores (to ensure that coverage by all chains in the sample is maintained)\nWith this approach, we may end up with a lot of observations per category, or too few, in which case a little tuning is necessary.\nEach item given a weight representing its relative importance\n\nStep 2: A monthly price index (i.e. relative) for each item is calculated as the unit value in the current month and unit value in the base year.\nStep 3: An elementary price index as the weighted average of each item’s relatives using the Laspeyres (as is done on at the higher level aggregation).\n\nChain the indices at December to create a long-term series\nOngoing work to replace:\n\nIf there is a big shift (but no exit), a product could be replaced manually using the bridged overlap method. Explicit quality adjustment used only when needed in a select number of cases. Implicit methods (typically the bridged overlap method) are used. See CPI Manual, chapter 7 for more detail on how to maintain methods.\n\nIf an item disappeared – replace it with a similar one that was present in the base period.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#resources",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#resources",
    "title": "Sampling methods for scanner data",
    "section": "Resources",
    "text": "Resources\nThe following articles summarize the fixed sample method well\n\nLarsen (2014) Implementing scanner data in the Danish CPI\nvan der Grient and Jan de Haan (2010) The use of supermarket scanner data in the Dutch CPI",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#the-concept-1",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#the-concept-1",
    "title": "Sampling methods for scanner data",
    "section": "The concept",
    "text": "The concept\n\nAs the distribution of expenditures is typically quite skewed, a relatively small number of items are usually representing the majority of expenditures. The method focuses on these using a cut-off-sampling approach and allows the basket (sample) to be updated (i.e. re-sampled) every month. Put differently, “an item will be used in the scomputation of hte index between two consecutive months if its average expenditure share (with respect tho the set of matched items) in those months is above a certain threshold value” (van der Grient and Jan de Haan, 2010).\nAn unweighted (i.e. not using turnover) Jevons is used and chained, thereby reducing the risk of chain drift (as weights are used implicitly for sampling of items but not explicitly for index calculation).\nThe other added benefit is that it shifts to an automatic way to maintain the sample, as the process to maintain the fixed sample method is also too labor intensive and hard to extend to many retailers (6 in the Netherlands case).\nThe dynamic method was also found to closely approximate the GEKS, hence its a useful way to measure food and non-food categories. After a while, the NSO can shift to the multilateral method is possible (requires a minimum of 13 months, ideally 25).\nImplementation requires research on dumping and outlier filters prior to implementing.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#step-by-step-process-1",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#step-by-step-process-1",
    "title": "Sampling methods for scanner data",
    "section": "Step-by-step process",
    "text": "Step-by-step process\n\nStep 1: Calculate movement for each product in the category based on the unit value price in this month over unit value price the previous month.\n\nPre-processing is needed to calculate unit prices\nFiltering/cleaning is also applied:\n\nOutlier filter excludes anything 300% higher and 75% decline\nDumping filter is used to exclude strong simultaneous price and quantity decreases – as this dumping in prices without an offsetting price increase will have a downward effect on the index.\n\nThe unique identifier applicable (EAN in the Dutch example) is used for matched-model method, but cleaning/investigation needed to make sure identifier is stable (sometimes retailers recycle EANs for instance). SKU can be used where applicable as it may be more stable (sometimes capture the relaunch problem). If the identifier is too detailed for CPI purposes (say where two products with different codes are identical from a consumers’ point of view), these should be grouped in theory but in principle can be explicitly quality adjusted.\nOnly the first several weeks of data are used – the Dutch case its 3 weeks, consistent with field collection\n\nStep 2: Use a cut off method to select a products with most of the sales – as in general a relatively small proportion of products will be responsible for the majority of expenditures and carry the most weight in weighted indices. The threshold chosen in the Dutch case was that 50% of the items are selected to represent 80-85% of expenditures\n\n\\(\\frac{s_{t}+s_{t+1}}{2}&gt;\\frac{1}{n\\times\\lambda}\\)\nWhere\n\\(\\frac{(s_t+s_{t+1})}{2}\\) – the product’s average share between the two periods\n\\(n\\) – the number of products\n\\(\\lambda\\) – 1.25\nFor example if \\(n\\) = 80 then items with an average expenditure share greater than 1% are selected\n\nStep 3: If item is missing – impute its price once (as you may have temporarily missing) by multiplying the last observed price by the Jevons of the category movement (i.e. ‘class mean imputation’ method). This is needed as a strict matched-item method will exclude temporary observed items from the computation.\nStep 4: If explicit adjustments needed such as package changes – can be made manually\n\nExamples are change in package sizes that are really the same for the consumer (i.e. EANs are basically too detailed, and we need a way to link/group 2 products together).\nOther implicit quality adjustment stays as per the matched model approach\n\nStep 5: Integrate retailer specific COICOP6 indices together using annual chained Laspeyres method (i.e. higher-level aggregation). Scanner data can be used to weigh different retailers (when present) but sub-COICOP6 weight may be needed to separate out non-scanner data sources that use a different aggregation method.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#categories-applicable",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#categories-applicable",
    "title": "Sampling methods for scanner data",
    "section": "Categories applicable",
    "text": "Categories applicable\nThe Dutch case showcased the application of the method for several COICOP categories, specifically Food (01), Wine and Beer (0212, 0213), Tools, household maintenance tools (055, 056), medical and pharmaceutical products (061), pet food (0934), and personal care products (1313). This showcases the application of the method.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#resources-1",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#resources-1",
    "title": "Sampling methods for scanner data",
    "section": "Resources",
    "text": "Resources\nThe following articles go through the process in depth\n\nvan der Grient and Jan de Haan (2010) The use of supermarket scanner data in the Dutch CPI\nvan der Grient and Jan de Haan (2011) Scanner Data Price Indexes: The “Dutch Method” versus Rolling Year GEKS",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "notebooks/aizcorbe_example_2.1.html",
    "href": "notebooks/aizcorbe_example_2.1.html",
    "title": "Aizcorbe (2014), ex 2.1. Bilateral fixed-base and chained indices",
    "section": "",
    "text": "This notebook goes through a fixed-base and chained bilateral indices methods demonstrated by Aizcorbe (2014) in 2.1. This this example Ana Aizcorbe uses US National Income and Product Accounts data from the BEA to demonstrate how the Laspeyres, Paasche, and Fisher behave differently when you use a fixed base versus when they are chained.\n\nimport pandas as pd\nimport numpy as np\nfrom PriceIndexCalc.pandas_modules.index_methods import bilateral_methods\n\n\n#if not run before, use the helper to clean the data and make it analysis ready\nfrom src.nipa_helper import clean_nipa_data\n\nclean_nipa_data()\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[82], line 2\n      1 #if not run before, use the helper to clean the data and make it analysis ready\n----&gt; 2 from src.nipa_helper import clean_nipa_data\n      4 clean_nipa_data()\n\nModuleNotFoundError: No module named 'src'\n\n\n\n\n\nWhile the raw data is a tad hard to work with (and looks awkward in Table 2.3), this data is provided in cleaned csv format in the data\\bronze folder\n\ndf = pd.read_csv(\"../data/silver/NIPA_ard.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYears\nid\nquantity\nprice\n\n\n\n\n0\n0\n2000\nMotor Vehicles and Parts\n363.2\n102.00\n\n\n1\n1\n2001\nMotor Vehicles and Parts\n383.3\n102.40\n\n\n2\n2\n2002\nMotor Vehicles and Parts\n401.3\n101.86\n\n\n3\n3\n2003\nMotor Vehicles and Parts\n401.0\n99.08\n\n\n4\n4\n2004\nMotor Vehicles and Parts\n403.9\n98.40\n\n\n\n\n\n\n\nTry the base bilateral indices\n\nbilateral_methods(\n    df_merged, \n    price_col='index',\n    quantity_col='spending',\n    product_id_col='product_name',\n    date_col='Years',\n    method='fisher',\n    plot=True\n    )\n\n\n\n\n\n\n\n\nindex_value\n\n\n\n\n2000\n1.000000\n\n\n2001\n0.976995\n\n\n2002\n0.950021\n\n\n2003\n0.914880\n\n\n2004\n0.895824\n\n\n2005\n0.885266\n\n\n2006\n0.870273\n\n\n2007\n0.854862\n\n\n2008\n0.837859\n\n\n2009\n0.827529\n\n\n2010\n0.822435\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry the Laspeyres\n\nbilateral_methods(\n    df_merged, \n    price_col='index',\n    quantity_col='spending',\n    product_id_col='product_name',\n    date_col='Years',\n    method='laspeyres',\n    plot=True\n    )\n\n\n\n\n\n\n\n\nindex_value\n\n\n\n\n2000\n1.000000\n\n\n2001\n0.976905\n\n\n2002\n0.949844\n\n\n2003\n0.915297\n\n\n2004\n0.897443\n\n\n2005\n0.888531\n\n\n2006\n0.875934\n\n\n2007\n0.861710\n\n\n2008\n0.847423\n\n\n2009\n0.837340\n\n\n2010\n0.834129"
  },
  {
    "objectID": "notebooks/aizcorbe_example_2.1.html#explore-the-data",
    "href": "notebooks/aizcorbe_example_2.1.html#explore-the-data",
    "title": "Aizcorbe (2014), ex 2.1. Bilateral fixed-base and chained indices",
    "section": "",
    "text": "While the raw data is a tad hard to work with (and looks awkward in Table 2.3), this data is provided in cleaned csv format in the data\\bronze folder\n\ndf = pd.read_csv(\"../data/silver/NIPA_ard.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYears\nid\nquantity\nprice\n\n\n\n\n0\n0\n2000\nMotor Vehicles and Parts\n363.2\n102.00\n\n\n1\n1\n2001\nMotor Vehicles and Parts\n383.3\n102.40\n\n\n2\n2\n2002\nMotor Vehicles and Parts\n401.3\n101.86\n\n\n3\n3\n2003\nMotor Vehicles and Parts\n401.0\n99.08\n\n\n4\n4\n2004\nMotor Vehicles and Parts\n403.9\n98.40\n\n\n\n\n\n\n\nTry the base bilateral indices\n\nbilateral_methods(\n    df_merged, \n    price_col='index',\n    quantity_col='spending',\n    product_id_col='product_name',\n    date_col='Years',\n    method='fisher',\n    plot=True\n    )\n\n\n\n\n\n\n\n\nindex_value\n\n\n\n\n2000\n1.000000\n\n\n2001\n0.976995\n\n\n2002\n0.950021\n\n\n2003\n0.914880\n\n\n2004\n0.895824\n\n\n2005\n0.885266\n\n\n2006\n0.870273\n\n\n2007\n0.854862\n\n\n2008\n0.837859\n\n\n2009\n0.827529\n\n\n2010\n0.822435\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry the Laspeyres\n\nbilateral_methods(\n    df_merged, \n    price_col='index',\n    quantity_col='spending',\n    product_id_col='product_name',\n    date_col='Years',\n    method='laspeyres',\n    plot=True\n    )\n\n\n\n\n\n\n\n\nindex_value\n\n\n\n\n2000\n1.000000\n\n\n2001\n0.976905\n\n\n2002\n0.949844\n\n\n2003\n0.915297\n\n\n2004\n0.897443\n\n\n2005\n0.888531\n\n\n2006\n0.875934\n\n\n2007\n0.861710\n\n\n2008\n0.847423\n\n\n2009\n0.837340\n\n\n2010\n0.834129"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html",
    "href": "notebooks/File_type_benchmark.html",
    "title": "Benchmark test in file formats",
    "section": "",
    "text": "When doing data analysis with big data, scaling is often a concern as the files we are working with are large. Hence we want to select file formats that are appropriate - have low on-disk usage and having fast input-output (i.e. read-write). This workbook does a benchmark assessment of a few well known file types. It is quite similar to other benchmark studies, such as this ‘towards data science’ format study by Ilia Zaitsev in 2017."
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#file-formats-analyzed",
    "href": "notebooks/File_type_benchmark.html#file-formats-analyzed",
    "title": "Benchmark test in file formats",
    "section": "File formats analyzed",
    "text": "File formats analyzed\nOld school file formats: 1. Pain CSV 2. Excel (xlsx)\nApache Arrow formats: 3. Parquet 4. Feather\nPython specific formats: 5. Pickle 6. Compressed pickle (using zip format)\nOther data formats: 7. HDF5"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#variables-used-in-the-analysis",
    "href": "notebooks/File_type_benchmark.html#variables-used-in-the-analysis",
    "title": "Benchmark test in file formats",
    "section": "Variables used in the analysis",
    "text": "Variables used in the analysis\n\ntime to save a file (output to stored memory);\ntime to load a file (input from stored memory);\nspace on disk"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#dataset-utilized-in-analyis",
    "href": "notebooks/File_type_benchmark.html#dataset-utilized-in-analyis",
    "title": "Benchmark test in file formats",
    "section": "Dataset utilized in analyis",
    "text": "Dataset utilized in analyis\nTo analyze each file format, the relatively large dataset from Dominiks’ Fine foods scanner dataset was utilized as it contains a range of variables - numeric, string, and integer – as well as being large (7.3M rows)."
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#note",
    "href": "notebooks/File_type_benchmark.html#note",
    "title": "Benchmark test in file formats",
    "section": "Note",
    "text": "Note\nAs a side comment, while this analysis was done on Python, the conclusions are mostly applicable to R or other langauges exclusing the use of Pickle and compressed pickle formats, which are python formats.\n\n# %matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\n\nSetup a simple timer to return time taken for a task\n\nimport datetime\n\nclass Timer:\n    \"\"\"\n    Simple timer. When first initiated, it starts, has one method stop(), it prints the time taken\n    \n    `\n    t = Timer()\n    t.stop()\n    `\n    \n    By default, the Timer() will just print the length of time taken, however if you specify\n    The stop(return_time=True), it will instead return a datetime object of time taken \n    \"\"\"\n    def __init__(self):\n        # when first initiated, start the clock\n        self.t_start = datetime.datetime.now()\n        \n    def stop(self, return_time=False):\n        # end timer\n        self.t_end = datetime.datetime.now()\n        # return or print the length of time taken\n        if return_time == True:\n            return self.t_end - self.t_start\n        else:\n            print(\"Task took {t}\".format(t=self.t_end - self.t_start))\n\nLoad the dataset used in the demo and see how long it is\nNOTE: at this step, any .csv demo dataset can be substituted\n\ndemo_dataset = \"https://www.chicagobooth.edu/-/media/enterprise/centers/kilts/datasets/dominicks-dataset/movement_csv-files/wana_csv.zip\"\n\ndf = pd.read_csv(demo_dataset)\nlen(df)\n\n7339217\n\n\nConsidering that there are 7.3M rows of data, this invalidates xlsx as the simple (or full) output type as this can only handle 1 million rows. If we had still wanted to work with excel for such a large dataset, we would have to split it into several 1M row files. Hence to simulate excel, we will just focus on saving or reading one 1M row file. To equalize the analysis at the end of the day, we will simply multiply the time taken to load/save/store 1m rows by 7.3\n\nexcel_muliplier = len(df)/1000000"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#setup-process-to-automate-the-tests",
    "href": "notebooks/File_type_benchmark.html#setup-process-to-automate-the-tests",
    "title": "Benchmark test in file formats",
    "section": "Setup Process to automate the tests",
    "text": "Setup Process to automate the tests\n\ndef execute_command(message, command):\n    print(\"starting test:\",message)\n    total = None\n    for each in range(0,4):\n        t = Timer()\n        exec(command)\n        if total == None:\n            total = t.stop(return_time=True)\n        else:\n            total = total + t.stop(return_time=True)\n    print(\"took {l}\".format(l=total/5))\n    time_av = total/5\n    # since excel can only store 1M records, multiply the excel estimate by the datasize ratio\n    if message[:4] == \"xlsx\":\n        time_av = time_av * excel_muliplier\n    return message, time_av.total_seconds()\n\n\nformats = {\n    \"pickle - write\":\"df.to_pickle('{}'.format(file_paths['pickle']))\",\n    \"pickle - read\":\"df = pd.read_pickle('{}'.format(file_paths['pickle']))\",\n    \"compressed pickle - write\":\"df.to_pickle('{}'.format(file_paths['compressed pickle']), compression='zip')\",\n    \"compressed pickle - read\":\"df = pd.read_pickle('{}'.format(file_paths['compressed pickle']))\",\n    \"csv - write\":\"df.to_csv('{}'.format(file_paths['csv']))\",\n    \"csv - read\":\"df = pd.read_csv('{}'.format(file_paths['csv']))\",\n    \"parquet - write\":\"df.to_parquet('{}'.format(file_paths['parquet']))\",\n    \"parquet - read\":\"df = pd.read_parquet('{}'.format(file_paths['parquet']))\",\n    \"feather - write\":\"df.to_feather('{}'.format(file_paths['feather']))\",\n    \"feather - read\":\"df = pd.read_feather('{}'.format(file_paths['feather']))\",\n    \"hdf5 - write\":\"df.to_hdf('{}'.format(file_paths['hdf5']), key='df')\",\n    \"hdf5 - read\":\"df = pd.read_hdf('{}'.format(file_paths['hdf5']))\",\n    \"xlsx - write\":\"df[:1000000].to_excel('{}'.format(file_paths['xlsx']))\",\n    \"xlsx - read\":\"df = pd.read_excel('{}'.format(file_paths['xlsx']))\"\n}\n\nfile_paths = {\n    \"csv\":\"wana.csv\",\n    \"pickle\":\"wana.pkl\",\n    \"compressed pickle\":\"wana.pkl.zip\",\n    \"feather\":\"wana.feather\",\n    \"parquet\":\"wana.parquet.gzip\",\n    \"hdf5\":\"wana.h5\",\n    \"xlsx\":\"wana.xlsx\"\n}"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#run-the-process-on-the-downloaded-demo-file",
    "href": "notebooks/File_type_benchmark.html#run-the-process-on-the-downloaded-demo-file",
    "title": "Benchmark test in file formats",
    "section": "Run the process on the downloaded demo file",
    "text": "Run the process on the downloaded demo file\n\ntimes = []\nfor _format in formats:\n    times.append(execute_command(_format, formats[_format]))\n\ndf_processing_test = pd.DataFrame(times, columns=[\"task\", \"time taken\"])\n\nstarting test: pickle - write\ntook 0:00:03.344310\nstarting test: pickle - read\ntook 0:00:01.997607\nstarting test: compressed pickle - write\ntook 0:00:07.953267\nstarting test: compressed pickle - read\ntook 0:00:12.442646\nstarting test: csv - write\ntook 0:00:40.201943\nstarting test: csv - read\ntook 0:00:06.481310\nstarting test: parquet - write\ntook 0:00:04.330219\nstarting test: parquet - read\ntook 0:00:03.246392\nstarting test: feather - write\ntook 0:00:02.894822\nstarting test: feather - read\ntook 0:00:01.323537\nstarting test: hdf5 - write\n\n\nC:\\Users\\gouss\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py:2449: PerformanceWarning: \nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type-&gt;mixed,key-&gt;block2_values] [items-&gt;Index(['SALE', 'PRICE_HEX', 'PROFIT_HEX'], dtype='object')]\n\n  encoding=encoding,\n\n\ntook 0:00:03.991776\nstarting test: hdf5 - read\ntook 0:00:05.148564\nstarting test: xlsx - write\ntook 0:02:35.857042\nstarting test: xlsx - read\ntook 0:01:36.240834\n\n\n\nfile_size_test = []\n\nfor file in file_paths:\n    fsize = os.path.getsize(file_paths[file])\n    # since excel can only store 1M records, multiply the excel estimate by the datasize ratio\n    if file[:4] == \"xlsx\":\n        fsize = fsize * excel_muliplier\n    file_size_test.append((file, fsize/1024/1024))\n\ndf_file_size_test = pd.DataFrame(file_size_test, columns=['type','file size (MB)'])"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#visualize-and-print-results-for-analysis",
    "href": "notebooks/File_type_benchmark.html#visualize-and-print-results-for-analysis",
    "title": "Benchmark test in file formats",
    "section": "Visualize and print results for analysis",
    "text": "Visualize and print results for analysis\nNow that all the results are availible, lets visualize them a bit prior to qualitative analysis\n\nplt.figure(figsize=(15,7))\nax = sns.barplot(x='type', y='file size (MB)', data=df_file_size_test)\n_ = ax.set_xlabel('File Format')\n_ = ax.set_ylabel('Megabytes')\n_ = ax.set_title('File size of the data that is saved')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large')\n\n[Text(0, 0, 'csv'),\n Text(0, 0, 'pickle'),\n Text(0, 0, 'compressed pickle'),\n Text(0, 0, 'feather'),\n Text(0, 0, 'parquet'),\n Text(0, 0, 'hdf5'),\n Text(0, 0, 'xlsx')]\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(20,10))\nax = sns.barplot(x='task', y='time taken', data=df_processing_test)\n_ = ax.set_xlabel('File format')\n_ = ax.set_ylabel('Time Taken (seconds)')\n_ = ax.set_title('File formats and the time taken for IO')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large', horizontalalignment='right')\n\n[Text(0, 0, 'pickle - write'),\n Text(0, 0, 'pickle - read'),\n Text(0, 0, 'compressed pickle - write'),\n Text(0, 0, 'compressed pickle - read'),\n Text(0, 0, 'csv - write'),\n Text(0, 0, 'csv - read'),\n Text(0, 0, 'parquet - write'),\n Text(0, 0, 'parquet - read'),\n Text(0, 0, 'feather - write'),\n Text(0, 0, 'feather - read'),\n Text(0, 0, 'hdf5 - write'),\n Text(0, 0, 'hdf5 - read'),\n Text(0, 0, 'xlsx - write'),\n Text(0, 0, 'xlsx - read')]\n\n\n\n\n\n\n\n\n\nOkay… so hdf5 and xlsx are clearly not favourable. If we drop them, what does the result look like\n\ndf_file_size_test_without_outliers = df_file_size_test.drop([5,6])\nplt.figure(figsize=(15,7))\nax = sns.barplot(x='type', y='file size (MB)', data=df_file_size_test_without_outliers)\n_ = ax.set_xlabel('File Format')\n_ = ax.set_ylabel('Megabytes')\n_ = ax.set_title('File size of the data that is saved\\n(dropped hdf5 as outlier)')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large')\n\n[Text(0, 0, 'csv'),\n Text(0, 0, 'pickle'),\n Text(0, 0, 'compressed pickle'),\n Text(0, 0, 'feather'),\n Text(0, 0, 'parquet')]\n\n\n\n\n\n\n\n\n\n\ndf_processing_test_without_outliers = df_processing_test.drop([10,11,12,13])\nplt.figure(figsize=(20,10))\nax = sns.barplot(x='task', y='time taken', data=df_processing_test_without_outliers)\n_ = ax.set_xlabel('File format')\n_ = ax.set_ylabel('Time Taken (seconds)')\n_ = ax.set_title('File formats and the time taken for IO\\n(dropped excel as extreme outlier)')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large', horizontalalignment='right')\n\n[Text(0, 0, 'pickle - write'),\n Text(0, 0, 'pickle - read'),\n Text(0, 0, 'compressed pickle - write'),\n Text(0, 0, 'compressed pickle - read'),\n Text(0, 0, 'csv - write'),\n Text(0, 0, 'csv - read'),\n Text(0, 0, 'parquet - write'),\n Text(0, 0, 'parquet - read'),\n Text(0, 0, 'feather - write'),\n Text(0, 0, 'feather - read')]"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#detailed-results",
    "href": "notebooks/File_type_benchmark.html#detailed-results",
    "title": "Benchmark test in file formats",
    "section": "Detailed results",
    "text": "Detailed results\nWe can print out the dataframes of interst to see the detailed data for all the files, including the outliers\n\ndf_file_size_test\n\n\n\n\n\n\n\n\ntype\nfile size (MB)\n\n\n\n\n0\ncsv\n547.119179\n\n\n1\npickle\n579.979185\n\n\n2\ncompressed pickle\n14.979172\n\n\n3\nfeather\n104.476992\n\n\n4\nparquet\n20.218006\n\n\n5\nhdf5\n2088.185272\n\n\n6\nxlsx\n288.665235\n\n\n\n\n\n\n\n\ndf_processing_test\n\n\n\n\n\n\n\n\ntask\ntime taken\n\n\n\n\n0\npickle - write\n3.344310\n\n\n1\npickle - read\n1.997607\n\n\n2\ncompressed pickle - write\n7.953267\n\n\n3\ncompressed pickle - read\n12.442646\n\n\n4\ncsv - write\n40.201943\n\n\n5\ncsv - read\n6.481310\n\n\n6\nparquet - write\n4.330219\n\n\n7\nparquet - read\n3.246392\n\n\n8\nfeather - write\n2.894822\n\n\n9\nfeather - read\n1.323537\n\n\n10\nhdf5 - write\n3.991776\n\n\n11\nhdf5 - read\n5.148564\n\n\n12\nxlsx - write\n1143.868652\n\n\n13\nxlsx - read\n706.332365"
  }
]