[
  {
    "objectID": "notebooks/aizcorbe_example_2.2_draft.html",
    "href": "notebooks/aizcorbe_example_2.2_draft.html",
    "title": "Aizcorbe (2014), ex 2.2. Bilateral indices",
    "section": "",
    "text": "Example of bilateral indices with product churn (from Aizcorbe 2014, example 2.2)\nIn Example 2.2, Aizcorbe uses the DRAM dataset to demonstrate how to construct bilateral price indices with product churn.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html",
    "href": "notebooks/File_type_benchmark.html",
    "title": "Benchmark test in file formats",
    "section": "",
    "text": "When doing data analysis with big data, scaling is often a concern as the files we are working with are large. Hence we want to select file formats that are appropriate - have low on-disk usage and having fast input-output (i.e. read-write). This workbook does a benchmark assessment of a few well known file types. It is quite similar to other benchmark studies, such as this ‘towards data science’ format study by Ilia Zaitsev in 2017."
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#file-formats-analyzed",
    "href": "notebooks/File_type_benchmark.html#file-formats-analyzed",
    "title": "Benchmark test in file formats",
    "section": "File formats analyzed",
    "text": "File formats analyzed\nOld school file formats: 1. Pain CSV 2. Excel (xlsx)\nApache Arrow formats: 3. Parquet 4. Feather\nPython specific formats: 5. Pickle 6. Compressed pickle (using zip format)\nOther data formats: 7. HDF5"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#variables-used-in-the-analysis",
    "href": "notebooks/File_type_benchmark.html#variables-used-in-the-analysis",
    "title": "Benchmark test in file formats",
    "section": "Variables used in the analysis",
    "text": "Variables used in the analysis\n\ntime to save a file (output to stored memory);\ntime to load a file (input from stored memory);\nspace on disk"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#dataset-utilized-in-analyis",
    "href": "notebooks/File_type_benchmark.html#dataset-utilized-in-analyis",
    "title": "Benchmark test in file formats",
    "section": "Dataset utilized in analyis",
    "text": "Dataset utilized in analyis\nTo analyze each file format, the relatively large dataset from Dominiks’ Fine foods scanner dataset was utilized as it contains a range of variables - numeric, string, and integer – as well as being large (7.3M rows)."
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#note",
    "href": "notebooks/File_type_benchmark.html#note",
    "title": "Benchmark test in file formats",
    "section": "Note",
    "text": "Note\nAs a side comment, while this analysis was done on Python, the conclusions are mostly applicable to R or other langauges exclusing the use of Pickle and compressed pickle formats, which are python formats.\n\n# %matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\n\nSetup a simple timer to return time taken for a task\n\nimport datetime\n\nclass Timer:\n    \"\"\"\n    Simple timer. When first initiated, it starts, has one method stop(), it prints the time taken\n    \n    `\n    t = Timer()\n    t.stop()\n    `\n    \n    By default, the Timer() will just print the length of time taken, however if you specify\n    The stop(return_time=True), it will instead return a datetime object of time taken \n    \"\"\"\n    def __init__(self):\n        # when first initiated, start the clock\n        self.t_start = datetime.datetime.now()\n        \n    def stop(self, return_time=False):\n        # end timer\n        self.t_end = datetime.datetime.now()\n        # return or print the length of time taken\n        if return_time == True:\n            return self.t_end - self.t_start\n        else:\n            print(\"Task took {t}\".format(t=self.t_end - self.t_start))\n\nLoad the dataset used in the demo and see how long it is\nNOTE: at this step, any .csv demo dataset can be substituted\n\ndemo_dataset = \"https://www.chicagobooth.edu/-/media/enterprise/centers/kilts/datasets/dominicks-dataset/movement_csv-files/wana_csv.zip\"\n\ndf = pd.read_csv(demo_dataset)\nlen(df)\n\n7339217\n\n\nConsidering that there are 7.3M rows of data, this invalidates xlsx as the simple (or full) output type as this can only handle 1 million rows. If we had still wanted to work with excel for such a large dataset, we would have to split it into several 1M row files. Hence to simulate excel, we will just focus on saving or reading one 1M row file. To equalize the analysis at the end of the day, we will simply multiply the time taken to load/save/store 1m rows by 7.3\n\nexcel_muliplier = len(df)/1000000"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#setup-process-to-automate-the-tests",
    "href": "notebooks/File_type_benchmark.html#setup-process-to-automate-the-tests",
    "title": "Benchmark test in file formats",
    "section": "Setup Process to automate the tests",
    "text": "Setup Process to automate the tests\n\ndef execute_command(message, command):\n    print(\"starting test:\",message)\n    total = None\n    for each in range(0,4):\n        t = Timer()\n        exec(command)\n        if total == None:\n            total = t.stop(return_time=True)\n        else:\n            total = total + t.stop(return_time=True)\n    print(\"took {l}\".format(l=total/5))\n    time_av = total/5\n    # since excel can only store 1M records, multiply the excel estimate by the datasize ratio\n    if message[:4] == \"xlsx\":\n        time_av = time_av * excel_muliplier\n    return message, time_av.total_seconds()\n\n\nformats = {\n    \"pickle - write\":\"df.to_pickle('{}'.format(file_paths['pickle']))\",\n    \"pickle - read\":\"df = pd.read_pickle('{}'.format(file_paths['pickle']))\",\n    \"compressed pickle - write\":\"df.to_pickle('{}'.format(file_paths['compressed pickle']), compression='zip')\",\n    \"compressed pickle - read\":\"df = pd.read_pickle('{}'.format(file_paths['compressed pickle']))\",\n    \"csv - write\":\"df.to_csv('{}'.format(file_paths['csv']))\",\n    \"csv - read\":\"df = pd.read_csv('{}'.format(file_paths['csv']))\",\n    \"parquet - write\":\"df.to_parquet('{}'.format(file_paths['parquet']))\",\n    \"parquet - read\":\"df = pd.read_parquet('{}'.format(file_paths['parquet']))\",\n    \"feather - write\":\"df.to_feather('{}'.format(file_paths['feather']))\",\n    \"feather - read\":\"df = pd.read_feather('{}'.format(file_paths['feather']))\",\n    \"hdf5 - write\":\"df.to_hdf('{}'.format(file_paths['hdf5']), key='df')\",\n    \"hdf5 - read\":\"df = pd.read_hdf('{}'.format(file_paths['hdf5']))\",\n    \"xlsx - write\":\"df[:1000000].to_excel('{}'.format(file_paths['xlsx']))\",\n    \"xlsx - read\":\"df = pd.read_excel('{}'.format(file_paths['xlsx']))\"\n}\n\nfile_paths = {\n    \"csv\":\"wana.csv\",\n    \"pickle\":\"wana.pkl\",\n    \"compressed pickle\":\"wana.pkl.zip\",\n    \"feather\":\"wana.feather\",\n    \"parquet\":\"wana.parquet.gzip\",\n    \"hdf5\":\"wana.h5\",\n    \"xlsx\":\"wana.xlsx\"\n}"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#run-the-process-on-the-downloaded-demo-file",
    "href": "notebooks/File_type_benchmark.html#run-the-process-on-the-downloaded-demo-file",
    "title": "Benchmark test in file formats",
    "section": "Run the process on the downloaded demo file",
    "text": "Run the process on the downloaded demo file\n\ntimes = []\nfor _format in formats:\n    times.append(execute_command(_format, formats[_format]))\n\ndf_processing_test = pd.DataFrame(times, columns=[\"task\", \"time taken\"])\n\nstarting test: pickle - write\ntook 0:00:03.344310\nstarting test: pickle - read\ntook 0:00:01.997607\nstarting test: compressed pickle - write\ntook 0:00:07.953267\nstarting test: compressed pickle - read\ntook 0:00:12.442646\nstarting test: csv - write\ntook 0:00:40.201943\nstarting test: csv - read\ntook 0:00:06.481310\nstarting test: parquet - write\ntook 0:00:04.330219\nstarting test: parquet - read\ntook 0:00:03.246392\nstarting test: feather - write\ntook 0:00:02.894822\nstarting test: feather - read\ntook 0:00:01.323537\nstarting test: hdf5 - write\n\n\nC:\\Users\\gouss\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py:2449: PerformanceWarning: \nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type-&gt;mixed,key-&gt;block2_values] [items-&gt;Index(['SALE', 'PRICE_HEX', 'PROFIT_HEX'], dtype='object')]\n\n  encoding=encoding,\n\n\ntook 0:00:03.991776\nstarting test: hdf5 - read\ntook 0:00:05.148564\nstarting test: xlsx - write\ntook 0:02:35.857042\nstarting test: xlsx - read\ntook 0:01:36.240834\n\n\n\nfile_size_test = []\n\nfor file in file_paths:\n    fsize = os.path.getsize(file_paths[file])\n    # since excel can only store 1M records, multiply the excel estimate by the datasize ratio\n    if file[:4] == \"xlsx\":\n        fsize = fsize * excel_muliplier\n    file_size_test.append((file, fsize/1024/1024))\n\ndf_file_size_test = pd.DataFrame(file_size_test, columns=['type','file size (MB)'])"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#visualize-and-print-results-for-analysis",
    "href": "notebooks/File_type_benchmark.html#visualize-and-print-results-for-analysis",
    "title": "Benchmark test in file formats",
    "section": "Visualize and print results for analysis",
    "text": "Visualize and print results for analysis\nNow that all the results are availible, lets visualize them a bit prior to qualitative analysis\n\nplt.figure(figsize=(15,7))\nax = sns.barplot(x='type', y='file size (MB)', data=df_file_size_test)\n_ = ax.set_xlabel('File Format')\n_ = ax.set_ylabel('Megabytes')\n_ = ax.set_title('File size of the data that is saved')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large')\n\n[Text(0, 0, 'csv'),\n Text(0, 0, 'pickle'),\n Text(0, 0, 'compressed pickle'),\n Text(0, 0, 'feather'),\n Text(0, 0, 'parquet'),\n Text(0, 0, 'hdf5'),\n Text(0, 0, 'xlsx')]\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(20,10))\nax = sns.barplot(x='task', y='time taken', data=df_processing_test)\n_ = ax.set_xlabel('File format')\n_ = ax.set_ylabel('Time Taken (seconds)')\n_ = ax.set_title('File formats and the time taken for IO')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large', horizontalalignment='right')\n\n[Text(0, 0, 'pickle - write'),\n Text(0, 0, 'pickle - read'),\n Text(0, 0, 'compressed pickle - write'),\n Text(0, 0, 'compressed pickle - read'),\n Text(0, 0, 'csv - write'),\n Text(0, 0, 'csv - read'),\n Text(0, 0, 'parquet - write'),\n Text(0, 0, 'parquet - read'),\n Text(0, 0, 'feather - write'),\n Text(0, 0, 'feather - read'),\n Text(0, 0, 'hdf5 - write'),\n Text(0, 0, 'hdf5 - read'),\n Text(0, 0, 'xlsx - write'),\n Text(0, 0, 'xlsx - read')]\n\n\n\n\n\n\n\n\n\nOkay… so hdf5 and xlsx are clearly not favourable. If we drop them, what does the result look like\n\ndf_file_size_test_without_outliers = df_file_size_test.drop([5,6])\nplt.figure(figsize=(15,7))\nax = sns.barplot(x='type', y='file size (MB)', data=df_file_size_test_without_outliers)\n_ = ax.set_xlabel('File Format')\n_ = ax.set_ylabel('Megabytes')\n_ = ax.set_title('File size of the data that is saved\\n(dropped hdf5 as outlier)')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large')\n\n[Text(0, 0, 'csv'),\n Text(0, 0, 'pickle'),\n Text(0, 0, 'compressed pickle'),\n Text(0, 0, 'feather'),\n Text(0, 0, 'parquet')]\n\n\n\n\n\n\n\n\n\n\ndf_processing_test_without_outliers = df_processing_test.drop([10,11,12,13])\nplt.figure(figsize=(20,10))\nax = sns.barplot(x='task', y='time taken', data=df_processing_test_without_outliers)\n_ = ax.set_xlabel('File format')\n_ = ax.set_ylabel('Time Taken (seconds)')\n_ = ax.set_title('File formats and the time taken for IO\\n(dropped excel as extreme outlier)')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large', horizontalalignment='right')\n\n[Text(0, 0, 'pickle - write'),\n Text(0, 0, 'pickle - read'),\n Text(0, 0, 'compressed pickle - write'),\n Text(0, 0, 'compressed pickle - read'),\n Text(0, 0, 'csv - write'),\n Text(0, 0, 'csv - read'),\n Text(0, 0, 'parquet - write'),\n Text(0, 0, 'parquet - read'),\n Text(0, 0, 'feather - write'),\n Text(0, 0, 'feather - read')]"
  },
  {
    "objectID": "notebooks/File_type_benchmark.html#detailed-results",
    "href": "notebooks/File_type_benchmark.html#detailed-results",
    "title": "Benchmark test in file formats",
    "section": "Detailed results",
    "text": "Detailed results\nWe can print out the dataframes of interst to see the detailed data for all the files, including the outliers\n\ndf_file_size_test\n\n\n\n\n\n\n\n\ntype\nfile size (MB)\n\n\n\n\n0\ncsv\n547.119179\n\n\n1\npickle\n579.979185\n\n\n2\ncompressed pickle\n14.979172\n\n\n3\nfeather\n104.476992\n\n\n4\nparquet\n20.218006\n\n\n5\nhdf5\n2088.185272\n\n\n6\nxlsx\n288.665235\n\n\n\n\n\n\n\n\ndf_processing_test\n\n\n\n\n\n\n\n\ntask\ntime taken\n\n\n\n\n0\npickle - write\n3.344310\n\n\n1\npickle - read\n1.997607\n\n\n2\ncompressed pickle - write\n7.953267\n\n\n3\ncompressed pickle - read\n12.442646\n\n\n4\ncsv - write\n40.201943\n\n\n5\ncsv - read\n6.481310\n\n\n6\nparquet - write\n4.330219\n\n\n7\nparquet - read\n3.246392\n\n\n8\nfeather - write\n2.894822\n\n\n9\nfeather - read\n1.323537\n\n\n10\nhdf5 - write\n3.991776\n\n\n11\nhdf5 - read\n5.148564\n\n\n12\nxlsx - write\n1143.868652\n\n\n13\nxlsx - read\n706.332365"
  },
  {
    "objectID": "notebooks/aizcorbe_example_2.1.html",
    "href": "notebooks/aizcorbe_example_2.1.html",
    "title": "Aizcorbe (2014), ex 2.1. Bilateral fixed-base and chained indices",
    "section": "",
    "text": "This notebook goes through a fixed-base and chained bilateral indices methods demonstrated by Aizcorbe (2014) in 2.1. This this example Ana Aizcorbe uses US National Income and Product Accounts data from the BEA to demonstrate how the Laspeyres, Paasche, and Fisher behave differently when you use a fixed base versus when they are chained.\n\nimport pandas as pd\nimport numpy as np\nfrom PriceIndexCalc.pandas_modules.index_methods import bilateral_methods\n\n\n# #if not run before, use the helper to clean the data and make it analysis ready\n# from src.nipa_helper import clean_nipa_data\n\n# clean_nipa_data()\n\n\n\nWhile the raw data is a tad hard to work with (and looks awkward in Table 2.3), this data is provided in cleaned csv format in the data\\bronze folder\n\ndf = pd.read_csv(\"../data/silver/NIPA_ard.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYears\nid\nquantity\nprice\n\n\n\n\n0\n0\n2000\nMotor Vehicles and Parts\n363.2\n102.00\n\n\n1\n1\n2001\nMotor Vehicles and Parts\n383.3\n102.40\n\n\n2\n2\n2002\nMotor Vehicles and Parts\n401.3\n101.86\n\n\n3\n3\n2003\nMotor Vehicles and Parts\n401.0\n99.08\n\n\n4\n4\n2004\nMotor Vehicles and Parts\n403.9\n98.40\n\n\n\n\n\n\n\nTry the base bilateral indices\n\nbilateral_methods(\n    df_merged, \n    price_col='index',\n    quantity_col='spending',\n    product_id_col='product_name',\n    date_col='Years',\n    method='fisher',\n    plot=True\n    )\n\n\n\n\n\n\n\n\nindex_value\n\n\n\n\n2000\n1.000000\n\n\n2001\n0.976995\n\n\n2002\n0.950021\n\n\n2003\n0.914880\n\n\n2004\n0.895824\n\n\n2005\n0.885266\n\n\n2006\n0.870273\n\n\n2007\n0.854862\n\n\n2008\n0.837859\n\n\n2009\n0.827529\n\n\n2010\n0.822435\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry the Laspeyres\n\nbilateral_methods(\n    df_merged, \n    price_col='index',\n    quantity_col='spending',\n    product_id_col='product_name',\n    date_col='Years',\n    method='laspeyres',\n    plot=True\n    )\n\n\n\n\n\n\n\n\nindex_value\n\n\n\n\n2000\n1.000000\n\n\n2001\n0.976905\n\n\n2002\n0.949844\n\n\n2003\n0.915297\n\n\n2004\n0.897443\n\n\n2005\n0.888531\n\n\n2006\n0.875934\n\n\n2007\n0.861710\n\n\n2008\n0.847423\n\n\n2009\n0.837340\n\n\n2010\n0.834129"
  },
  {
    "objectID": "notebooks/aizcorbe_example_2.1.html#explore-the-data",
    "href": "notebooks/aizcorbe_example_2.1.html#explore-the-data",
    "title": "Aizcorbe (2014), ex 2.1. Bilateral fixed-base and chained indices",
    "section": "",
    "text": "While the raw data is a tad hard to work with (and looks awkward in Table 2.3), this data is provided in cleaned csv format in the data\\bronze folder\n\ndf = pd.read_csv(\"../data/silver/NIPA_ard.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYears\nid\nquantity\nprice\n\n\n\n\n0\n0\n2000\nMotor Vehicles and Parts\n363.2\n102.00\n\n\n1\n1\n2001\nMotor Vehicles and Parts\n383.3\n102.40\n\n\n2\n2\n2002\nMotor Vehicles and Parts\n401.3\n101.86\n\n\n3\n3\n2003\nMotor Vehicles and Parts\n401.0\n99.08\n\n\n4\n4\n2004\nMotor Vehicles and Parts\n403.9\n98.40\n\n\n\n\n\n\n\nTry the base bilateral indices\n\nbilateral_methods(\n    df_merged, \n    price_col='index',\n    quantity_col='spending',\n    product_id_col='product_name',\n    date_col='Years',\n    method='fisher',\n    plot=True\n    )\n\n\n\n\n\n\n\n\nindex_value\n\n\n\n\n2000\n1.000000\n\n\n2001\n0.976995\n\n\n2002\n0.950021\n\n\n2003\n0.914880\n\n\n2004\n0.895824\n\n\n2005\n0.885266\n\n\n2006\n0.870273\n\n\n2007\n0.854862\n\n\n2008\n0.837859\n\n\n2009\n0.827529\n\n\n2010\n0.822435\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry the Laspeyres\n\nbilateral_methods(\n    df_merged, \n    price_col='index',\n    quantity_col='spending',\n    product_id_col='product_name',\n    date_col='Years',\n    method='laspeyres',\n    plot=True\n    )\n\n\n\n\n\n\n\n\nindex_value\n\n\n\n\n2000\n1.000000\n\n\n2001\n0.976905\n\n\n2002\n0.949844\n\n\n2003\n0.915297\n\n\n2004\n0.897443\n\n\n2005\n0.888531\n\n\n2006\n0.875934\n\n\n2007\n0.861710\n\n\n2008\n0.847423\n\n\n2009\n0.837340\n\n\n2010\n0.834129"
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html",
    "href": "docs/notes_on_methods/bilateral_indices.html",
    "title": "Notes on bilateral price index methods",
    "section": "",
    "text": "This page provides an overview of Bilateral Price Index methods. Several sources are used (included in the links to the page) to go over key concepts. Emprical examples are linked from this page or can be found in the main guide\n\n\n\nA price index provides an aggregate measure of price change for a particular product segment, industry, or overall economy. … [They compare] the cost of purchasing a set of goods at different points in time. This “set of goods” is often referred to as the “market basket” or the “bundle” of goods. - Aizcorbe (2014)",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#the-concept-of-price-index-methods",
    "href": "docs/notes_on_methods/bilateral_indices.html#the-concept-of-price-index-methods",
    "title": "Notes on bilateral price index methods",
    "section": "",
    "text": "A price index provides an aggregate measure of price change for a particular product segment, industry, or overall economy. … [They compare] the cost of purchasing a set of goods at different points in time. This “set of goods” is often referred to as the “market basket” or the “bundle” of goods. - Aizcorbe (2014)",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#laspeyres-index",
    "href": "docs/notes_on_methods/bilateral_indices.html#laspeyres-index",
    "title": "Notes on bilateral price index methods",
    "section": "Laspeyres index",
    "text": "Laspeyres index\nThe simplest formula (and one that is commonly used) is the Laspeyres index:\n\\(I^{L}_{0,1} = \\frac{\\sum_{m=1}^{M}(P_{m,1}Q_{m,0})}{\\sum_{m=1}^{M}(P_{m,0}Q_{m,0})}\\)\nWhere P’s and Q’s denote prices and quantities, and 0 and 1 denote two points in time.\nIf M goods are sold in both periods (note that an overlap is needed), we can compare the the cost of purchasing the same goods we bought in period 1 with a certain period in the future.\nWe can also write the Laspeyres as the weighted arithmetic average of the price change of the individual products in the index\n\\(I^L_{0,1} = \\sum_{m=1}^{M} (w_{m,0}\\frac{P_{m,1}}{P_{m,0}})\\) such that \\(w_{m,0} = (P_{m,0}Q_{m,0})/\\sum_{m=1}^{M}(P_{m,0}Q_{m,0})\\)\ngive the ratio of good m’s expenditure to total expenditure, or could also be considered the relative importance or share of the product. There are some key nuances with this approach:\n\nProducts sold in both periods are included in both periods, thus new products are omitted.\nWe fix the relative importance of the goods for both periods based on period 1 weight, thus we do not reflect changes in composition over time (substitution). This can be convenient as we need weights only for the base period.\n\nMost price indices are variants of the Laspeyres, such as the Lowe (which compare the prices from the current month with the previous month, but use weights from a year before that)",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#paashe-index",
    "href": "docs/notes_on_methods/bilateral_indices.html#paashe-index",
    "title": "Notes on bilateral price index methods",
    "section": "Paashe Index",
    "text": "Paashe Index\nSimilar to the Lasperyes, however uses a different basket (the one from the pricing period:\n\\(I^P_{0,1} = \\sum_{m=1}^M (P_{m,1}Q_{m,1}) / \\sum_{m=1}^M\n(P_{m,0}Q_{m,1})\\) The Paasche may also be expressed as a function of the weighted average (i.e. shares):\n\\(I^P_{0,1} = 1/ \\sum_{m=1}^M (w_{m,1} \\frac{P_{m,0}}{P_{m,1}})\\) such that \\(w_{m,1} = (P_{m,1}Q_{m,1})/ \\sum_{m=1}^M (w_{m,1}P_{m,0}Q_{m,1})\\)\nThe Laspeyres and Paasche include prices and quantities in both periods, and both use the same relatives with different expenditure shares.",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#fisher-index",
    "href": "docs/notes_on_methods/bilateral_indices.html#fisher-index",
    "title": "Notes on bilateral price index methods",
    "section": "Fisher Index",
    "text": "Fisher Index\nThe Fisher does a geometric average of the Laspeyres and the Paasche:\n\\(I^F_{0,1} = (I^L_{0,1}I^P_{0,1})^\\frac{1}{2}\\)\nThe Fisher thus uses expenditure from both periods and thus provides relative importance that are more closely aligned with the goods actually sold. As the Fisher satisfies homogeneity, symmetry, and the time reversal test (the price change from the base to the current should be the inverse of the current to the base) - thus it doesn’t matter what period is chosen as the base.",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#törnqvist-index",
    "href": "docs/notes_on_methods/bilateral_indices.html#törnqvist-index",
    "title": "Notes on bilateral price index methods",
    "section": "Törnqvist Index",
    "text": "Törnqvist Index\nSimilar to the Fisher, however it takes the average of the weights instead of averaging the two indices\nIn logged form:\n\\(lnI^T_{0,1}=\\sum^M_{m=1}(w_{m,0}+w_{m,1})/2(ln\\frac{P_{m,1}}{P_{m,0}})\\)",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/bilateral_indices.html#jevons-index",
    "href": "docs/notes_on_methods/bilateral_indices.html#jevons-index",
    "title": "Notes on bilateral price index methods",
    "section": "Jevons Index",
    "text": "Jevons Index\nThe Jevons is unweighted geometric mean. Similar to the Törnqvist, in logged form:\n\\(lnI^J_{0,1} = \\frac{1}{M} \\sum^M_{m=1}ln(P_{m,1}/P_{m,0})\\)\nThe Jevons takes the unweighted average by replacing the \\((w_{m,0}+w_{m,1})/2\\) with \\(1/M\\), thus giving each model equal weight",
    "crumbs": [
      "Notes by method",
      "Notes on bilateral price index methods"
    ]
  },
  {
    "objectID": "docs/empirical_experiments/experiments_catalogue.html",
    "href": "docs/empirical_experiments/experiments_catalogue.html",
    "title": "Experiments catalogue",
    "section": "",
    "text": "This pages lists the experiments done by topic on specific topics related in some way to price statistics",
    "crumbs": [
      "Experiments catalogue"
    ]
  },
  {
    "objectID": "docs/empirical_experiments/experiments_catalogue.html#list-of-empirical-price-statistics-experiments",
    "href": "docs/empirical_experiments/experiments_catalogue.html#list-of-empirical-price-statistics-experiments",
    "title": "Experiments catalogue",
    "section": "List of empirical price statistics experiments",
    "text": "List of empirical price statistics experiments\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nCategories\n\n\n\n\n\n\n\n\nAizcorbe (2014), ex 2.1. Bilateral fixed-base and chained indices\n\n\nbilateral index methods, fixed-base indices, chained indices\n\n\n\n\n\n\nBenchmark test in file formats\n\n\ndata engineering, file format benchmarks\n\n\n\n\n\n\nExplore Italian web scraped grocery dataset\n\n\ndataset\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Experiments catalogue"
    ]
  },
  {
    "objectID": "docs/empirical_experiments/experiments/italian_web_scraped_dataset.html",
    "href": "docs/empirical_experiments/experiments/italian_web_scraped_dataset.html",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "",
    "text": "Let’s explore a web scrape dataset that Daniele Sasso and a few others made availible on Zenodo - https://doi.org/10.5281/zenodo.14927602\nFirst off - what is the shape and columns in the data:\n\n# Required libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# # Load dataset\n# df = pd.read_csv(\"../../../data/bronze/Variations_Food_Prices_Italian_Supermarkets_raw.csv\")\n# # df.head()\n\n# print(df.info())\n\nIts a tabular dataset. Lets look at the web scrape data in a bit more detail:\n\n# stats = {}\n# stats['Number of unique products'] = df['product'].nunique()\n# stats['Number of unique stores'] = df['store_id'].nunique()\n# stats['Number of unique regions'] = df['region'].nunique()\n# stats['Number of COICOP5 categories'] = df['COICOP5'].nunique()\n# stats['Number of unique scrapes'] = df['date'].nunique()\n# stats['Number of average unique products per store per date'] = round(df.groupby([\"date\", \"store_id\"])[\"product_id\"].nunique().reset_index()['product_id'].mean(),1)\n# d_end = datetime.fromisoformat(df['date'].max())\n# d_start = datetime.fromisoformat(df['date'].min())\n# d = d_end-d_start\n# stats['number of days in sample'] = d.days\n# pd.DataFrame.from_dict(stats, orient='index', columns=['statistic'])\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Overview",
    "section": "",
    "text": "This is a personal site that I used to render experiments and workflows that relate to the price statistics domain within official statistics. It is meant to log various experiments and notes to myself in a reproducible manner. Anyone else can use this if they choose but it is not explicitly meant for any formal capacity.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/index.html#purpose-of-site",
    "href": "docs/index.html#purpose-of-site",
    "title": "Overview",
    "section": "",
    "text": "This is a personal site that I used to render experiments and workflows that relate to the price statistics domain within official statistics. It is meant to log various experiments and notes to myself in a reproducible manner. Anyone else can use this if they choose but it is not explicitly meant for any formal capacity.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html",
    "href": "docs/notes_on_methods/sampling_with_scanner.html",
    "title": "Sampling methods for scanner data",
    "section": "",
    "text": "Even when scanner data is available, not all data necessarily needs to be used to calculate elementary price indices. There are two sampling approaches that are typically used as they are quite effective ways to use a subset of the data but - the static and dynamic sample methods. A requirement for sampling is that each unique product is already classified to categories within which the elementary price indices are calculated.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#the-concept",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#the-concept",
    "title": "Sampling methods for scanner data",
    "section": "The concept",
    "text": "The concept\n\nThe fixed sample method simulates a traditional approach, similar to field collection. The sample is refreshed every year to stay representative and replacement is used to maintain the sample following a traditional methodology (except that scanner data is available to sample from, i.e. you can seeing all products and all transactions, which provides a lot of additional detail of what product to replace with).\nThe method is simple (in the sense that it follows a traditional methodology and can be combined with other data in a traditional way), however it is resource intensive as replacement must be dealt with manually (for example the Netherlands found it too costly to extend to 6 scanner retailers and shifted to the dynamic sample method). It is also typically used with only a small (although representative) sample of data and can be the first approach used when starting out with scanner data.\nThe fixed sample method favors comparability over representativity as the same items are compared over time while the representativity of the sample deteriorates over time (which is only partially mitigated by annual product resampling):\n\nA consequence of the method is thus that only similar products are selected for most disappearing items (to avoid quality adjustment), which means the method is most applicable to situations where quality change is not a major factor in the category.\nThe method is also only applicable to categories with little churn as representativity of the products in sample will decline quickly if this is not the case. Its best to not use this method too widely due to lower representativity.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#step-by-step-process",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#step-by-step-process",
    "title": "Sampling methods for scanner data",
    "section": "Step-by-step process",
    "text": "Step-by-step process\nThe summary of the overall process is best summarized by the Eurostat (2017) guide, chapter 6:\n\nIn the static approach, a sample is drawn from year t and used for 12 months following December of year t. The sample is kept, and replacements are made as needed.\n\nIn a more detailed sense, the following steps are used to select and maintain the sample:\n\nStep 1: For each category (typically COICOP 6) for which an elementary price index will be calculated, select a number of items (representative products) at the beginning of the year which were representative the previous year. For instance, products that constituted the top 50% of turnover (i.e. using cut-off sampling) for the reference month, several months, or whole year can be included:\n\nSelect unique products (such as by European Article Number (EAN)) per supermarket chain and stores (to ensure that coverage by all chains in the sample is maintained)\nWith this approach, we may end up with a lot of observations per category, or too few, in which case a little tuning is necessary.\nEach item given a weight representing its relative importance.\n\nStep 2: A monthly price index (i.e. relative) for each item is calculated as the unit value in the current month and unit value in the base year.\nStep 3: An elementary price index is calculated using item relatives in the traditional way. Two ways can be used:\n\nA Jevons approach, the same as field collection data for elementary aggregates where price relatives are calculated at the representative product by region level.\nA weighted Laspeyres approach using each item’s relatives as input, which is akin to higher level aggregation.\n\nOther considerations:\n\nFor the Laspeyres type approach, chain the short-term indices on the December month to create a long-term series.\nOutlier methods as well as a dumping filter should be implemented to detect and remove unusual prices.\nImputation can be used for one or two months before a replacement is selected.\nIf an item is considered permanently missing or it is found to not be representative anymore, a new item needs to be selected that is not yet in the basket but was present in the base period. For instance, if there is a big shift (but no exit), a product could be replaced manually. Quality adjustment may be necessary (see CPI Manual, chapter 7 for more detail on how to maintain methods):\n\nExplicit quality adjustment used only when needed in a select number of cases (such as if there is a change in the content of an item, quantity adjustment can be used).\nImplicit methods (typically the overlap method, but also mean imputation) may also need to be used.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#resources",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#resources",
    "title": "Sampling methods for scanner data",
    "section": "Resources",
    "text": "Resources\nThe following articles summarize the fixed sample method well\n\nLarsen (2014) Implementing scanner data in the Danish CPI\nvan der Grient and Jan de Haan (2010) The use of supermarket scanner data in the Dutch CPI. Provides a good summary of the Laspeyres approach of the fixed sample method.\nLamboray (2024) Ukraine: Technical Assistance Report - Report on Consumer Price Index Mission. Provides a detailed summary of the Jevons approach to the fixed sample method.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#the-concept-1",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#the-concept-1",
    "title": "Sampling methods for scanner data",
    "section": "The concept",
    "text": "The concept\n\nAs the distribution of expenditures is typically quite skewed, a relatively small number of items are usually representing the majority of expenditures. The method focuses on these using a cut-off-sampling approach and allows the basket (sample) to be updated (i.e. re-sampled) every month. Put differently, “an item will be used in the computation of the index between two consecutive months if its average expenditure share (with respect to the set of matched items) in those months is above a certain threshold value” (van der Grient and Jan de Haan, 2010).\nAn unweighted (i.e. not using turnover) Jevons is used and chained, thereby reducing the risk of chain drift (as weights are used implicitly for sampling of items but not explicitly for index calculation).\nThe other added benefit is that it shifts to an automatic way to maintain the sample, as the process to maintain the fixed sample method is also too labor intensive and hard to extend to many retailers (6 in the Netherlands case).\nThe dynamic method was also found to closely approximate the GEKS, hence its a useful way to measure food and non-food categories. After a while, the NSO can shift to the multilateral method (requires a minimum of 13 months, ideally 25).\nImplementation requires choosing appropriate dumping and outlier filters prior to implementing.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#step-by-step-process-1",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#step-by-step-process-1",
    "title": "Sampling methods for scanner data",
    "section": "Step-by-step process",
    "text": "Step-by-step process\n\nStep 1: Calculate movement (price relative) for each product in the category based on the unit value price in this month over unit value price the previous month.\n\nPre-processing is needed to calculate unit prices.\nFiltering/cleaning is also applied:\n\nFor instance, an outlier filter could exclude anything 300% higher and 75% decline.\nA dumping filter could be used to exclude strong simultaneous price and quantity decreases – as this dumping in prices without an offsetting price increase will have a downward effect on the index.\n\nThe applicable unique identifier (EAN in the Dutch example) is used for matched-model method, but cleaning/investigation needed to make sure identifier is stable (sometimes retailers recycle EANs for instance). SKU can be used where applicable as it may be more stable (sometimes capture the relaunch problem). If the identifier is too detailed for CPI purposes (say where two products with different codes are identical from a consumers’ point of view), these should be grouped in theory but in principle can be explicitly quality adjusted.\nOnly the first several weeks of data are used – the Dutch case its 3 weeks, consistent with field collection.\n\nStep 2: Use a cut-off method to select products that represent a most of the sales in the category – general, a relatively small proportion of products will be responsible for the majority of expenditures and would carry the most weight in weighted indices (hence the cut-off method prioritizes the same products). The threshold chosen in the Dutch case was that 50% of the items are selected to represent 80-85% of expenditures\n\n     \\(\\frac{s_{t}+s_{t-1}}{2}&gt;\\frac{1}{n\\times\\lambda}\\)\n     Where \\(\\frac{(s_t+s_{t-1})}{2}\\) is the product’s average share between the two periods\n     \\(n\\) – the number of products\n     \\(\\lambda\\) – 1.25\n     For example, if \\(n\\) = 80 then items with an average expenditure share greater than 1% are selected\n\nStep 3: If an item is missing – impute its price once by multiplying the last observed price by the Jevons of the category movement (i.e. ‘class mean imputation’ method), i.e. the output of step 5 below. This is needed as a strict matched-item method will exclude temporary observed items from the computation.\nStep 4: If explicit adjustments needed such as package changes – can be made manually:\n\nExamples are change in package sizes that are really the same for the consumer (i.e. EANs are basically too detailed, and we need a way to link/group 2 products together).\n\nStep 5: Aggregate the price relatives in sample for this and the previous month using a Jevons (unweighted) index method at the elementary level. The month-to-month Jevons is chained to obtain a long-term time series.\nStep 6: Integrate retailer specific COICOP6 indices together using annual chained Laspeyres method (i.e. higher-level aggregation). Scanner data can be used to weigh different retailers (when present) but sub-COICOP6 weight may be needed to separate out non-scanner data sources that use a different aggregation method.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#categories-applicable",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#categories-applicable",
    "title": "Sampling methods for scanner data",
    "section": "Categories applicable",
    "text": "Categories applicable\nThe Dutch case showcased the application of the method for several COICOP categories, specifically Food (01), Wine and Beer (0212, 0213), Tools, household maintenance tools (055, 056), medical and pharmaceutical products (061), pet food (0934), and personal care products (1313). This showcases the application of the method.",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "docs/notes_on_methods/sampling_with_scanner.html#resources-1",
    "href": "docs/notes_on_methods/sampling_with_scanner.html#resources-1",
    "title": "Sampling methods for scanner data",
    "section": "Resources",
    "text": "Resources\nThe following articles go through the process in depth\n\nvan der Grient and Jan de Haan (2010) The use of supermarket scanner data in the Dutch CPI\nvan der Grient and Jan de Haan (2011) Scanner Data Price Indexes: The “Dutch Method” versus Rolling Year GEKS",
    "crumbs": [
      "Notes by method",
      "Sampling methods for scanner data"
    ]
  },
  {
    "objectID": "notebooks/aizcorbe_example_3.2_draft.html",
    "href": "notebooks/aizcorbe_example_3.2_draft.html",
    "title": "Aizcorbe (2014), ex 3.2. Hedonic dummy variable method",
    "section": "",
    "text": "Example of dummy variable hedonic method (from Aizcorbe 2014, example 3.2)\nIn Example 3.2, constructed a sythentic scanner dataset\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/Sasso_et_al_italian_dataset.html",
    "href": "notebooks/Sasso_et_al_italian_dataset.html",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "",
    "text": "Daniele Sasso and a few others made their dataset availible on Zenodo - https://doi.org/10.5281/zenodo.14927602 - daily webscraped data from different shops of an Italian supermarket chain. This blog summarizes the dataset and explores its various facets. Detailed overview of the data is available on the Price Stats Catalogue record of this dataset and some explorations below are summarized there."
  },
  {
    "objectID": "notebooks/Sasso_et_al_italian_dataset.html#dataset-structure",
    "href": "notebooks/Sasso_et_al_italian_dataset.html#dataset-structure",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Dataset structure",
    "text": "Dataset structure"
  },
  {
    "objectID": "notebooks/Sasso_et_al_italian_dataset.html#general-overview-of-the-dataset",
    "href": "notebooks/Sasso_et_al_italian_dataset.html#general-overview-of-the-dataset",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "General overview of the dataset",
    "text": "General overview of the dataset\nFirst off - lets look at the data itself, its columns, and some statistics about the web scraping itself.\n\n\n\n\n\n\n\n\n\ndate\nprice\nproduct_id\nstore_id\nregion\nproduct\nCOICOP5\nCOICOP4\n\n\n\n\n0\n2020-12-03\n1.99\n2\n2\ncalabria\narance navelina italia calibro 1.5 kg\nOranges\nFruit\n\n\n1\n2020-12-03\n2.48\n2\n3\nlazio\narance navelina italia calibro 1.5 kg\nOranges\nFruit\n\n\n2\n2020-12-03\n2.49\n2\n4\ncalabria\narance navelina italia calibro 1.5 kg\nOranges\nFruit\n\n\n3\n2020-12-03\n1.99\n2\n5\ncalabria\narance navelina italia calibro 1.5 kg\nOranges\nFruit\n\n\n4\n2020-12-03\n2.49\n2\n8\nlazio\narance navelina italia calibro 1.5 kg\nOranges\nFruit\n\n\n\n\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4033211 entries, 0 to 4033210\nData columns (total 8 columns):\n #   Column      Dtype  \n---  ------      -----  \n 0   date        object \n 1   price       float64\n 2   product_id  int64  \n 3   store_id    int64  \n 4   region      object \n 5   product     object \n 6   COICOP5     object \n 7   COICOP4     object \ndtypes: float64(1), int64(2), object(5)\nmemory usage: 246.2+ MB"
  },
  {
    "objectID": "notebooks/Sasso_et_al_italian_dataset.html#detailed-info-about-the-dataset",
    "href": "notebooks/Sasso_et_al_italian_dataset.html#detailed-info-about-the-dataset",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Detailed info about the dataset",
    "text": "Detailed info about the dataset\nAs this is web scrape data for several years - its saved all in one analytical table.\nLet’s look at it in a bit more detail:\n\n\n\n\n\n\n\n\n\nstatistic\n\n\n\n\nNumber of unique products\n2361.0\n\n\nNumber of unique stores\n20.0\n\n\nNumber of unique regions\n7.0\n\n\nNumber of COICOP5 categories\n24.0\n\n\nNumber of unique scrapes\n841.0\n\n\nNumber of average unique products per store per date\n236.0\n\n\nnumber of days in sample\n863.0\n\n\n\n\n\n\n\nIt seems that there are 863 days but 841 scrapes - that means that there were no scrapes during 22 days:\n\n\nDatetimeIndex(['2021-02-13', '2021-02-14', '2021-03-23', '2021-03-28',\n               '2021-06-24', '2021-08-22', '2021-08-23', '2021-08-24',\n               '2021-08-25', '2021-08-26', '2021-08-27', '2021-09-30',\n               '2021-12-02', '2022-01-23', '2022-03-16', '2022-06-21',\n               '2022-10-01', '2022-10-07', '2022-10-10', '2022-10-22',\n               '2022-10-23', '2022-11-04'],\n              dtype='datetime64[ns]', freq=None)\n\n\nIf we pivot the raw data and show the number of prices captured per store per region - it looks like this:\n\n\n\n\n\n\n\n\n\nCOICOP4\n...\nproduct_id\n\n\nregion\ncalabria\ncampania\nemilia-romagna\nlazio\n...\nlazio\nlombardia\nsicilia\numbria\n\n\nstore_id\n2\n4\n5\n11\n12\n18\n20\n7\n15\n3\n...\n8\n19\n21\n6\n9\n10\n14\n16\n13\n17\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-12-03\n244.0\n170.0\n152.0\n123.0\n210.0\n108.0\n309.0\n275.0\n378.0\n207.0\n...\n237.0\n128.0\n184.0\n366.0\n251.0\n238.0\n74.0\n305.0\n196.0\n255.0\n\n\n2020-12-04\n247.0\n174.0\n148.0\n123.0\n207.0\n108.0\n308.0\n348.0\n488.0\n207.0\n...\n235.0\n130.0\n181.0\n450.0\n315.0\n331.0\n112.0\n372.0\n195.0\n253.0\n\n\n2020-12-05\n243.0\n180.0\n147.0\n122.0\n207.0\n108.0\n299.0\n340.0\n495.0\n207.0\n...\n233.0\n130.0\n179.0\n448.0\n309.0\n323.0\n113.0\n370.0\n196.0\n251.0\n\n\n2020-12-06\n243.0\n180.0\n147.0\n122.0\n208.0\n108.0\n304.0\n347.0\n502.0\n207.0\n...\n233.0\n130.0\n179.0\n464.0\n320.0\n330.0\n130.0\n374.0\n196.0\n251.0\n\n\n2020-12-07\n242.0\n171.0\n148.0\n127.0\n208.0\n105.0\n304.0\n344.0\n512.0\n207.0\n...\n232.0\n133.0\n177.0\n470.0\n331.0\n339.0\n121.0\n381.0\n185.0\n253.0\n\n\n\n\n5 rows × 100 columns\n\n\n\nLets also look at the number of stores per region (i.e. the above but visually)"
  },
  {
    "objectID": "notebooks/Sasso_et_al_italian_dataset.html#trends-about-what-was-captured",
    "href": "notebooks/Sasso_et_al_italian_dataset.html#trends-about-what-was-captured",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Trends about what was captured",
    "text": "Trends about what was captured\n\nBy category\nLets look at the number of unique products and the number of web offers captured by COICOP5 category\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOver time\nWe can also consider how much data was captured across time\n\n\n\n\n\n\n\n\n\nIt seems that the amount of web offers started to decline. This should probably be investigated (if its region or store coverage) to see if longitudinal time series should exclude any of this data"
  },
  {
    "objectID": "notebooks/Sasso_et_al_italian_dataset.html#geographic-distribution-of-unique-products-by-region",
    "href": "notebooks/Sasso_et_al_italian_dataset.html#geographic-distribution-of-unique-products-by-region",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Geographic distribution of unique products by region",
    "text": "Geographic distribution of unique products by region\nThere is some example code in the zenodo page for the dataset that shows well some of the price/product info captured"
  },
  {
    "objectID": "notebooks/Sasso_et_al_italian_dataset.html#basic-analysis-average-price-trend-over-time-by-coicop4",
    "href": "notebooks/Sasso_et_al_italian_dataset.html#basic-analysis-average-price-trend-over-time-by-coicop4",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Basic analysis: average price trend over time (by COICOP4)",
    "text": "Basic analysis: average price trend over time (by COICOP4)\nWe can also look at average prices by COICOP4 over time"
  }
]